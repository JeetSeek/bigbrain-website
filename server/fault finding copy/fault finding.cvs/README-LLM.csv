filename,line_number,content
README-LLM.md,1,# LLM Integration for Heating System Fault Finding Database
README-LLM.md,2,
README-LLM.md,3,This document provides technical implementation details for integrating the heating system fault finding guides with Large Language Models (LLMs) using vector embeddings and structured data relationships.
README-LLM.md,4,
README-LLM.md,5,## Database Architecture
README-LLM.md,6,
README-LLM.md,7,### 1. Schema Design
README-LLM.md,8,
README-LLM.md,9,The database uses a hybrid approach combining vector embeddings for semantic search with structured relationships for precise querying:
README-LLM.md,10,
README-LLM.md,11,#### Core Tables Structure
README-LLM.md,12,
README-LLM.md,13,```sql
README-LLM.md,14,-- Components table
README-LLM.md,15,CREATE TABLE components (
README-LLM.md,16,"    id UUID PRIMARY KEY,"
README-LLM.md,17,"    name TEXT NOT NULL,"
README-LLM.md,18,"    component_type TEXT NOT NULL,"
README-LLM.md,19,"    description TEXT NOT NULL,"
README-LLM.md,20,"    applies_to TEXT[] NOT NULL,"
README-LLM.md,21,"    function TEXT NOT NULL,"
README-LLM.md,22,    embedding VECTOR(1536)  -- OpenAI embedding dimension
README-LLM.md,23,);
README-LLM.md,24,
README-LLM.md,25,-- Symptoms table
README-LLM.md,26,CREATE TABLE symptoms (
README-LLM.md,27,"    id UUID PRIMARY KEY,"
README-LLM.md,28,"    name TEXT NOT NULL,"
README-LLM.md,29,"    description TEXT NOT NULL,"
README-LLM.md,30,    embedding VECTOR(1536)
README-LLM.md,31,);
README-LLM.md,32,
README-LLM.md,33,-- Faults table
README-LLM.md,34,CREATE TABLE faults (
README-LLM.md,35,"    id UUID PRIMARY KEY,"
README-LLM.md,36,"    name TEXT NOT NULL,"
README-LLM.md,37,"    description TEXT NOT NULL,"
README-LLM.md,38,"    component_id UUID REFERENCES components(id),"
README-LLM.md,39,    embedding VECTOR(1536)
README-LLM.md,40,);
README-LLM.md,41,
README-LLM.md,42,-- Procedures table
README-LLM.md,43,CREATE TABLE procedures (
README-LLM.md,44,"    id UUID PRIMARY KEY,"
README-LLM.md,45,"    name TEXT NOT NULL,"
README-LLM.md,46,"    purpose TEXT NOT NULL,"
README-LLM.md,47,"    equipment_needed TEXT[] NOT NULL,"
README-LLM.md,48,"    steps JSONB NOT NULL,"
README-LLM.md,49,"    interpretation JSONB NOT NULL,"
README-LLM.md,50,    embedding VECTOR(1536)
README-LLM.md,51,);
README-LLM.md,52,
README-LLM.md,53,-- Relationships table
README-LLM.md,54,CREATE TABLE relationships (
README-LLM.md,55,"    id UUID PRIMARY KEY,"
README-LLM.md,56,"    from_type TEXT NOT NULL,"
README-LLM.md,57,"    from_id UUID NOT NULL,"
README-LLM.md,58,"    relationship_type TEXT NOT NULL,"
README-LLM.md,59,"    to_type TEXT NOT NULL,"
README-LLM.md,60,"    to_id UUID NOT NULL,"
README-LLM.md,61,"    strength TEXT NOT NULL,"
README-LLM.md,62,    conditions TEXT[]
README-LLM.md,63,);
README-LLM.md,64,
README-LLM.md,65,-- Document chunks table
README-LLM.md,66,CREATE TABLE document_chunks (
README-LLM.md,67,"    id UUID PRIMARY KEY,"
README-LLM.md,68,"    document_name TEXT NOT NULL,"
README-LLM.md,69,"    chunk_index INTEGER NOT NULL,"
README-LLM.md,70,"    content TEXT NOT NULL,"
README-LLM.md,71,"    metadata JSONB NOT NULL,"
README-LLM.md,72,    embedding VECTOR(1536)
README-LLM.md,73,);
README-LLM.md,74,```
README-LLM.md,75,
README-LLM.md,76,### 2. Vector Embedding Implementation
README-LLM.md,77,
README-LLM.md,78,#### Content Chunking Strategy
README-LLM.md,79,
README-LLM.md,80,Each document should be chunked into semantic units:
README-LLM.md,81,
README-LLM.md,82,1. **Size**: 512-1024 tokens per chunk (balancing detail and context)
README-LLM.md,83,2. **Overlap**: 50-100 tokens of overlap between chunks for context continuity
README-LLM.md,84,3. **Semantic Boundaries**: Prioritize chunking at section/subsection boundaries
README-LLM.md,85,
README-LLM.md,86,```python
README-LLM.md,87,"def chunk_document(document_path, chunk_size=800, overlap=100):"
README-LLM.md,88,"    """""""
README-LLM.md,89,    Chunk a markdown document into semantic sections with overlap
README-LLM.md,90,"    """""""
README-LLM.md,91,"    with open(document_path, 'r') as f:"
README-LLM.md,92,        content = f.read()
README-LLM.md,93,
README-LLM.md,94,    # Split by markdown headers to maintain semantic sections
README-LLM.md,95,"    sections = re.split(r'(#{1,6}\s+.*?)\n', content)"
README-LLM.md,96,
README-LLM.md,97,    chunks = []
README-LLM.md,98,"    current_chunk = """""
README-LLM.md,99,    current_size = 0
README-LLM.md,100,
README-LLM.md,101,"    for i in range(0, len(sections), 2):"
README-LLM.md,102,        if i+1 < len(sections):
README-LLM.md,103,            # Combine header with its content
README-LLM.md,104,            section = sections[i] + sections[i+1]
README-LLM.md,105,            section_size = len(section.split())
README-LLM.md,106,
README-LLM.md,107,            if current_size + section_size <= chunk_size:
README-LLM.md,108,                # Add to current chunk
README-LLM.md,109,                current_chunk += section
README-LLM.md,110,                current_size += section_size
README-LLM.md,111,            else:
README-LLM.md,112,                # Store current chunk and start new one with overlap
README-LLM.md,113,                chunks.append(current_chunk)
README-LLM.md,114,
README-LLM.md,115,                # Create overlap by finding a good boundary
README-LLM.md,116,"                overlap_text = get_overlap_text(current_chunk, overlap)"
README-LLM.md,117,                current_chunk = overlap_text + section
README-LLM.md,118,                current_size = len(current_chunk.split())
README-LLM.md,119,        else:
README-LLM.md,120,            # Handle trailing section
README-LLM.md,121,            current_chunk += sections[i]
README-LLM.md,122,
README-LLM.md,123,    # Add final chunk
README-LLM.md,124,    if current_chunk:
README-LLM.md,125,        chunks.append(current_chunk)
README-LLM.md,126,
README-LLM.md,127,    return chunks
README-LLM.md,128,```
README-LLM.md,129,
README-LLM.md,130,#### Embedding Generation
README-LLM.md,131,
README-LLM.md,132,Using OpenAI's text-embedding-3-small model (or equivalent):
README-LLM.md,133,
README-LLM.md,134,```python
README-LLM.md,135,import openai
README-LLM.md,136,from supabase import create_client
README-LLM.md,137,
README-LLM.md,138,# Initialize clients
README-LLM.md,139,"openai.api_key = ""your-openai-key"""
README-LLM.md,140,supabase = create_client(
README-LLM.md,141,"    SUPABASE_URL,"
README-LLM.md,142,    SUPABASE_SERVICE_KEY
README-LLM.md,143,)
README-LLM.md,144,
README-LLM.md,145,"async def generate_embeddings(chunks, document_name):"
README-LLM.md,146,"    """""""
README-LLM.md,147,    Generate embeddings for document chunks and store in Supabase
README-LLM.md,148,"    """""""
README-LLM.md,149,"    for i, chunk in enumerate(chunks):"
README-LLM.md,150,        # Generate embedding
README-LLM.md,151,        response = await openai.embeddings.create(
README-LLM.md,152,"            model=""text-embedding-3-small"","
README-LLM.md,153,            input=chunk
README-LLM.md,154,        )
README-LLM.md,155,        embedding = response.data[0].embedding
README-LLM.md,156,
README-LLM.md,157,        # Extract metadata
README-LLM.md,158,        metadata = extract_metadata(chunk)
README-LLM.md,159,
README-LLM.md,160,        # Store in Supabase
README-LLM.md,161,"        supabase.table(""document_chunks"").insert({"
README-LLM.md,162,"            ""document_name"": document_name,"
README-LLM.md,163,"            ""chunk_index"": i,"
README-LLM.md,164,"            ""content"": chunk,"
README-LLM.md,165,"            ""metadata"": metadata,"
README-LLM.md,166,"            ""embedding"": embedding"
README-LLM.md,167,        }).execute()
README-LLM.md,168,```
README-LLM.md,169,
README-LLM.md,170,### 3. Relationship Mapping
README-LLM.md,171,
README-LLM.md,172,Establish explicit connections between entities:
README-LLM.md,173,
README-LLM.md,174,```python
README-LLM.md,175,"def create_relationships(from_entity, relationship_type, to_entity, strength=""medium"", conditions=None):"
README-LLM.md,176,"    """""""
README-LLM.md,177,    Create relationship between two entities
README-LLM.md,178,"    """""""
README-LLM.md,179,"    supabase.table(""relationships"").insert({"
README-LLM.md,180,"        ""from_type"": from_entity[""type""],"
README-LLM.md,181,"        ""from_id"": from_entity[""id""],"
README-LLM.md,182,"        ""relationship_type"": relationship_type,"
README-LLM.md,183,"        ""to_type"": to_entity[""type""],"
README-LLM.md,184,"        ""to_id"": to_entity[""id""],"
README-LLM.md,185,"        ""strength"": strength,"
README-LLM.md,186,"        ""conditions"": conditions or []"
README-LLM.md,187,    }).execute()
README-LLM.md,188,```
README-LLM.md,189,
README-LLM.md,190,## Query Processing System
README-LLM.md,191,
README-LLM.md,192,### 1. Query Understanding
README-LLM.md,193,
README-LLM.md,194,Parse user queries to identify:
README-LLM.md,195,"- Query type (diagnostic, informational, procedural)"
README-LLM.md,196,"- Relevant entities (components, symptoms, procedures)"
README-LLM.md,197,"- Contextual constraints (system type, age, conditions)"
README-LLM.md,198,
README-LLM.md,199,```python
README-LLM.md,200,def process_query(user_query):
README-LLM.md,201,"    """""""
README-LLM.md,202,    Process a user query to extract intent and entities
README-LLM.md,203,"    """""""
README-LLM.md,204,    # Use LLM to classify query and extract entities
README-LLM.md,205,"    classification_prompt = f"""""""
README-LLM.md,206,"    Analyze this query about heating systems: ""{user_query}"""
README-LLM.md,207,
README-LLM.md,208,"    1. Query type (choose one): diagnostic, informational, procedural"
README-LLM.md,209,    2. Mentioned components: [list all heating components mentioned]
README-LLM.md,210,    3. Mentioned symptoms: [list all symptoms/problems mentioned]
README-LLM.md,211,"    4. System constraints: [list any system types, conditions, or limitations mentioned]"
README-LLM.md,212,"    5. Expected response type: [troubleshooting steps, component information, test procedure, etc.]"
README-LLM.md,213,
README-LLM.md,214,    Format as JSON.
README-LLM.md,215,"    """""""
README-LLM.md,216,
README-LLM.md,217,    response = openai.chat.completions.create(
README-LLM.md,218,"        model=""gpt-4"","
README-LLM.md,219,"        messages=[{""role"": ""user"", ""content"": classification_prompt}],"
README-LLM.md,220,"        response_format={""type"": ""json_object""}"
README-LLM.md,221,    )
README-LLM.md,222,
README-LLM.md,223,    query_analysis = json.loads(response.choices[0].message.content)
README-LLM.md,224,    return query_analysis
README-LLM.md,225,```
README-LLM.md,226,
README-LLM.md,227,### 2. Retrieval Strategy
README-LLM.md,228,
README-LLM.md,229,Different retrieval approaches based on query type:
README-LLM.md,230,
README-LLM.md,231,#### Diagnostic Queries
README-LLM.md,232,
README-LLM.md,233,```python
README-LLM.md,234,"async def retrieve_for_diagnostic(symptom, components=None, system_type=None):"
README-LLM.md,235,"    """""""
README-LLM.md,236,    Retrieve information for diagnostic queries
README-LLM.md,237,"    """""""
README-LLM.md,238,    # 1. Generate embedding for symptom
README-LLM.md,239,    symptom_embedding = await generate_embedding(symptom)
README-LLM.md,240,
README-LLM.md,241,    # 2. Find matching symptoms with vector search
README-LLM.md,242,    symptom_results = supabase.rpc(
README-LLM.md,243,"        ""match_symptoms"","
README-LLM.md,244,        {
README-LLM.md,245,"            ""query_embedding"": symptom_embedding,"
README-LLM.md,246,"            ""match_threshold"": 0.78,"
README-LLM.md,247,"            ""match_count"": 5"
README-LLM.md,248,        }
README-LLM.md,249,    ).execute()
README-LLM.md,250,
README-LLM.md,251,    # 3. Find related faults through relationships
README-LLM.md,252,"    symptom_ids = [s[""id""] for s in symptom_results.data]"
README-LLM.md,253,"    fault_relationships = supabase.table(""relationships"").select(""to_id"").eq(""from_type"", ""symptom"").in_(""from_id"", symptom_ids).eq(""to_type"", ""fault"").execute()"
README-LLM.md,254,
README-LLM.md,255,    # 4. Get detailed fault and component information
README-LLM.md,256,"    fault_ids = [r[""to_id""] for r in fault_relationships.data]"
README-LLM.md,257,
README-LLM.md,258,    # 5. Apply component and system type filters if provided
README-LLM.md,259,"    query = supabase.table(""faults"").select(""*"").in_(""id"", fault_ids)"
README-LLM.md,260,    if components:
README-LLM.md,261,        related_component_ids = get_component_ids(components)
README-LLM.md,262,"        query = query.in_(""component_id"", related_component_ids)"
README-LLM.md,263,
README-LLM.md,264,    faults = query.execute()
README-LLM.md,265,
README-LLM.md,266,    # 6. Get relevant procedural information
README-LLM.md,267,    procedures = get_relevant_procedures(fault_ids)
README-LLM.md,268,
README-LLM.md,269,    # 7. Get supporting document chunks
README-LLM.md,270,"    supporting_chunks = get_supporting_document_chunks(symptom, components)"
README-LLM.md,271,
README-LLM.md,272,    return {
README-LLM.md,273,"        ""symptoms"": symptom_results.data,"
README-LLM.md,274,"        ""faults"": faults.data,"
README-LLM.md,275,"        ""procedures"": procedures,"
README-LLM.md,276,"        ""supporting_content"": supporting_chunks"
README-LLM.md,277,    }
README-LLM.md,278,```
README-LLM.md,279,
README-LLM.md,280,#### Informational Queries
README-LLM.md,281,
README-LLM.md,282,```python
README-LLM.md,283,"async def retrieve_for_informational(component_name=None, topic=None):"
README-LLM.md,284,"    """""""
README-LLM.md,285,    Retrieve information for general information queries
README-LLM.md,286,"    """""""
README-LLM.md,287,    if component_name:
README-LLM.md,288,        # Retrieve component information
README-LLM.md,289,        component_embedding = await generate_embedding(component_name)
README-LLM.md,290,        components = supabase.rpc(
README-LLM.md,291,"            ""match_components"","
README-LLM.md,292,            {
README-LLM.md,293,"                ""query_embedding"": component_embedding,"
README-LLM.md,294,"                ""match_threshold"": 0.75,"
README-LLM.md,295,"                ""match_count"": 2"
README-LLM.md,296,            }
README-LLM.md,297,        ).execute()
README-LLM.md,298,
README-LLM.md,299,        # Get related document chunks
README-LLM.md,300,"        component_chunks = get_component_document_chunks(components.data[0][""id""])"
README-LLM.md,301,
README-LLM.md,302,        return {
README-LLM.md,303,"            ""component_info"": components.data,"
README-LLM.md,304,"            ""supporting_content"": component_chunks"
README-LLM.md,305,        }
README-LLM.md,306,
README-LLM.md,307,    if topic:
README-LLM.md,308,        # Search across all document chunks
README-LLM.md,309,        topic_embedding = await generate_embedding(topic)
README-LLM.md,310,        relevant_chunks = supabase.rpc(
README-LLM.md,311,"            ""match_documents"","
README-LLM.md,312,            {
README-LLM.md,313,"                ""query_embedding"": topic_embedding,"
README-LLM.md,314,"                ""match_threshold"": 0.70,"
README-LLM.md,315,"                ""match_count"": 8"
README-LLM.md,316,            }
README-LLM.md,317,        ).execute()
README-LLM.md,318,
README-LLM.md,319,        return {
README-LLM.md,320,"            ""topic_information"": relevant_chunks.data"
README-LLM.md,321,        }
README-LLM.md,322,```
README-LLM.md,323,
README-LLM.md,324,### 3. Response Generation
README-LLM.md,325,
README-LLM.md,326,Generate structured responses using LLM:
README-LLM.md,327,
README-LLM.md,328,```python
README-LLM.md,329,"async def generate_response(query_analysis, retrieved_data):"
README-LLM.md,330,"    """""""
README-LLM.md,331,    Generate a structured response using the LLM
README-LLM.md,332,"    """""""
README-LLM.md,333,    # Prepare context from retrieved data
README-LLM.md,334,    context = format_retrieval_context(retrieved_data)
README-LLM.md,335,
README-LLM.md,336,"    response_prompt = f"""""""
README-LLM.md,337,"    Query: {query_analysis[""original_query""]}"
README-LLM.md,338,
README-LLM.md,339,    Query Analysis: {json.dumps(query_analysis)}
README-LLM.md,340,
README-LLM.md,341,    Retrieved Information:
README-LLM.md,342,    {context}
README-LLM.md,343,
README-LLM.md,344,"    Based on the above information from our heating system fault finding database, provide a helpful and accurate response."
README-LLM.md,345,
README-LLM.md,346,    If diagnosing a problem:
README-LLM.md,347,    1. List the most likely causes based on symptoms
README-LLM.md,348,    2. Provide specific diagnostic steps in order of priority
README-LLM.md,349,    3. Explain how to confirm the exact fault
README-LLM.md,350,    4. Describe resolution approaches
README-LLM.md,351,
README-LLM.md,352,    If explaining a component or procedure:
README-LLM.md,353,    1. Provide a clear definition/explanation
README-LLM.md,354,    2. Include technical specifications if relevant
README-LLM.md,355,    3. Describe related components or systems
README-LLM.md,356,    4. Cover common issues or maintenance requirements
README-LLM.md,357,
README-LLM.md,358,"    Only include information supported by the retrieved data. If essential information is missing, indicate what additional details would help provide a more complete answer."
README-LLM.md,359,"    """""""
README-LLM.md,360,
README-LLM.md,361,    # Call LLM for response generation
README-LLM.md,362,    response = openai.chat.completions.create(
README-LLM.md,363,"        model=""gpt-4"","
README-LLM.md,364,"        messages=[{""role"": ""user"", ""content"": response_prompt}]"
README-LLM.md,365,    )
README-LLM.md,366,
README-LLM.md,367,    return response.choices[0].message.content
README-LLM.md,368,```
README-LLM.md,369,
README-LLM.md,370,## Supabase Implementation
README-LLM.md,371,
README-LLM.md,372,### 1. Vector Search Functions
README-LLM.md,373,
README-LLM.md,374,Create PostgreSQL functions for vector search:
README-LLM.md,375,
README-LLM.md,376,```sql
README-LLM.md,377,-- Create vector similarity search function
README-LLM.md,378,CREATE OR REPLACE FUNCTION match_documents(
README-LLM.md,379,"    query_embedding VECTOR(1536),"
README-LLM.md,380,"    match_threshold FLOAT,"
README-LLM.md,381,    match_count INT
README-LLM.md,382,)
README-LLM.md,383,RETURNS TABLE (
README-LLM.md,384,"    id UUID,"
README-LLM.md,385,"    document_name TEXT,"
README-LLM.md,386,"    chunk_index INT,"
README-LLM.md,387,"    content TEXT,"
README-LLM.md,388,"    metadata JSONB,"
README-LLM.md,389,    similarity FLOAT
README-LLM.md,390,)
README-LLM.md,391,LANGUAGE plpgsql
README-LLM.md,392,AS $$
README-LLM.md,393,BEGIN
README-LLM.md,394,    RETURN QUERY
README-LLM.md,395,    SELECT
README-LLM.md,396,"        document_chunks.id,"
README-LLM.md,397,"        document_chunks.document_name,"
README-LLM.md,398,"        document_chunks.chunk_index,"
README-LLM.md,399,"        document_chunks.content,"
README-LLM.md,400,"        document_chunks.metadata,"
README-LLM.md,401,        1 - (document_chunks.embedding <=> query_embedding) AS similarity
README-LLM.md,402,    FROM document_chunks
README-LLM.md,403,    WHERE 1 - (document_chunks.embedding <=> query_embedding) > match_threshold
README-LLM.md,404,    ORDER BY similarity DESC
README-LLM.md,405,    LIMIT match_count;
README-LLM.md,406,END;
README-LLM.md,407,$$;
README-LLM.md,408,
README-LLM.md,409,-- Create similar functions for other entity types
README-LLM.md,410,CREATE OR REPLACE FUNCTION match_components(
README-LLM.md,411,"    query_embedding VECTOR(1536),"
README-LLM.md,412,"    match_threshold FLOAT,"
README-LLM.md,413,    match_count INT
README-LLM.md,414,)
README-LLM.md,415,RETURNS TABLE (
README-LLM.md,416,"    id UUID,"
README-LLM.md,417,"    name TEXT,"
README-LLM.md,418,"    component_type TEXT,"
README-LLM.md,419,"    description TEXT,"
README-LLM.md,420,    similarity FLOAT
README-LLM.md,421,)
README-LLM.md,422,LANGUAGE plpgsql
README-LLM.md,423,AS $$
README-LLM.md,424,BEGIN
README-LLM.md,425,    RETURN QUERY
README-LLM.md,426,    SELECT
README-LLM.md,427,"        components.id,"
README-LLM.md,428,"        components.name,"
README-LLM.md,429,"        components.component_type,"
README-LLM.md,430,"        components.description,"
README-LLM.md,431,        1 - (components.embedding <=> query_embedding) AS similarity
README-LLM.md,432,    FROM components
README-LLM.md,433,    WHERE 1 - (components.embedding <=> query_embedding) > match_threshold
README-LLM.md,434,    ORDER BY similarity DESC
README-LLM.md,435,    LIMIT match_count;
README-LLM.md,436,END;
README-LLM.md,437,$$;
README-LLM.md,438,```
README-LLM.md,439,
README-LLM.md,440,### 2. Data Ingestion Pipeline
README-LLM.md,441,
README-LLM.md,442,Process for ingesting documents into the database:
README-LLM.md,443,
README-LLM.md,444,```javascript
README-LLM.md,445,// Example Node.js ingestion script
README-LLM.md,446,
README-LLM.md,447,const fs = require('fs');
README-LLM.md,448,const path = require('path');
README-LLM.md,449,const { createClient } = require('@supabase/supabase-js');
README-LLM.md,450,"const { Configuration, OpenAIApi } = require('openai');"
README-LLM.md,451,const { v4: uuidv4 } = require('uuid');
README-LLM.md,452,const dotenv = require('dotenv');
README-LLM.md,453,
README-LLM.md,454,dotenv.config();
README-LLM.md,455,
README-LLM.md,456,// Initialize clients
README-LLM.md,457,const supabase = createClient(
README-LLM.md,458,"    process.env.SUPABASE_URL,"
README-LLM.md,459,    process.env.SUPABASE_SERVICE_KEY
README-LLM.md,460,);
README-LLM.md,461,
README-LLM.md,462,const configuration = new Configuration({
README-LLM.md,463,"    apiKey: process.env.OPENAI_API_KEY,"
README-LLM.md,464,});
README-LLM.md,465,const openai = new OpenAIApi(configuration);
README-LLM.md,466,
README-LLM.md,467,async function processDocuments() {
README-LLM.md,468,    const docsDir = '/Users/markburrows/Desktop/fault finding/';
README-LLM.md,469,    const files = fs.readdirSync(docsDir).filter(file => file.endsWith('.md') && file !== 'README.md' && file !== 'README-LLM.md');
README-LLM.md,470,
README-LLM.md,471,    for (const file of files) {
README-LLM.md,472,        console.log(`Processing ${file}...`);
README-LLM.md,473,"        const filePath = path.join(docsDir, file);"
README-LLM.md,474,"        const content = fs.readFileSync(filePath, 'utf8');"
README-LLM.md,475,
README-LLM.md,476,        // Extract metadata
README-LLM.md,477,"        const metadata = extractMetadata(content, file);"
README-LLM.md,478,
README-LLM.md,479,        // Chunk the document
README-LLM.md,480,        const chunks = chunkDocument(content);
README-LLM.md,481,
README-LLM.md,482,        // Process each chunk
README-LLM.md,483,        for (let i = 0; i < chunks.length; i++) {
README-LLM.md,484,            const chunk = chunks[i];
README-LLM.md,485,
README-LLM.md,486,            // Generate embedding
README-LLM.md,487,            const embeddingResponse = await openai.createEmbedding({
README-LLM.md,488,"                model: ""text-embedding-3-small"","
README-LLM.md,489,"                input: chunk,"
README-LLM.md,490,            });
README-LLM.md,491,            const embedding = embeddingResponse.data.data[0].embedding;
README-LLM.md,492,
README-LLM.md,493,            // Store in Supabase
README-LLM.md,494,            await supabase.from('document_chunks').insert({
README-LLM.md,495,"                id: uuidv4(),"
README-LLM.md,496,"                document_name: file.replace('.md', ''),"
README-LLM.md,497,"                chunk_index: i,"
README-LLM.md,498,"                content: chunk,"
README-LLM.md,499,                metadata: {
README-LLM.md,500,"                    ...metadata,"
README-LLM.md,501,"                    chunk_type: getChunkType(chunk),"
README-LLM.md,502,                    entities: extractEntities(chunk)
README-LLM.md,503,"                },"
README-LLM.md,504,                embedding: embedding
README-LLM.md,505,            });
README-LLM.md,506,
README-LLM.md,507,            console.log(`Stored chunk ${i+1}/${chunks.length} for ${file}`);
README-LLM.md,508,
README-LLM.md,509,            // Respect rate limits
README-LLM.md,510,"            await new Promise(r => setTimeout(r, 200));"
README-LLM.md,511,        }
README-LLM.md,512,    }
README-LLM.md,513,}
README-LLM.md,514,
README-LLM.md,515,"function extractMetadata(content, filename) {"
README-LLM.md,516,"    // Extract system type, component type, etc."
README-LLM.md,517,    // This is a simplified example
README-LLM.md,518,"    const systemTypes = ['condensing_boiler', 'non_condensing_boiler', 'underfloor_heating', 'radiator_system', 'open_vent', 'sealed_system'];"
README-LLM.md,519,
README-LLM.md,520,    const metadata = {
README-LLM.md,521,"        document_type: getDocumentType(filename),"
README-LLM.md,522,"        system_types: systemTypes.filter(type => content.toLowerCase().includes(type.replace('_', ' '))),"
README-LLM.md,523,"        contains_procedures: content.includes('Diagnostic Procedures:'),"
README-LLM.md,524,"        contains_symptoms: content.includes('Symptoms:'),"
README-LLM.md,525,        contains_reference: content.includes('Reference') || content.includes('Table')
README-LLM.md,526,    };
README-LLM.md,527,
README-LLM.md,528,    return metadata;
README-LLM.md,529,}
README-LLM.md,530,
README-LLM.md,531,"// Additional helper functions for chunking, entity extraction, etc."
README-LLM.md,532,```
README-LLM.md,533,
README-LLM.md,534,## Integration with LLM Application
README-LLM.md,535,
README-LLM.md,536,### 1. Client-Side Implementation
README-LLM.md,537,
README-LLM.md,538,Example React component for querying the fault finding database:
README-LLM.md,539,
README-LLM.md,540,```jsx
README-LLM.md,541,"import React, { useState } from 'react';"
README-LLM.md,542,import { createClient } from '@supabase/supabase-js';
README-LLM.md,543,import axios from 'axios';
README-LLM.md,544,
README-LLM.md,545,// Initialize Supabase client
README-LLM.md,546,const supabaseUrl = process.env.REACT_APP_SUPABASE_URL;
README-LLM.md,547,const supabaseKey = process.env.REACT_APP_SUPABASE_ANON_KEY;
README-LLM.md,548,"const supabase = createClient(supabaseUrl, supabaseKey);"
README-LLM.md,549,
README-LLM.md,550,function FaultFindingAssistant() {
README-LLM.md,551,"    const [query, setQuery] = useState('');"
README-LLM.md,552,"    const [response, setResponse] = useState('');"
README-LLM.md,553,"    const [loading, setLoading] = useState(false);"
README-LLM.md,554,
README-LLM.md,555,    async function handleSubmit(e) {
README-LLM.md,556,        e.preventDefault();
README-LLM.md,557,        setLoading(true);
README-LLM.md,558,
README-LLM.md,559,        try {
README-LLM.md,560,            // Process query via API
README-LLM.md,561,"            const result = await axios.post('/api/fault-finding/query', { query });"
README-LLM.md,562,            setResponse(result.data.response);
README-LLM.md,563,        } catch (error) {
README-LLM.md,564,"            console.error('Error querying fault-finding database:', error);"
README-LLM.md,565,            setResponse('Error processing your query. Please try again.');
README-LLM.md,566,        } finally {
README-LLM.md,567,            setLoading(false);
README-LLM.md,568,        }
README-LLM.md,569,    }
README-LLM.md,570,
README-LLM.md,571,    return (
README-LLM.md,572,"        <div className=""fault-finding-assistant"">"
README-LLM.md,573,            <h2>Heating System Fault Finding Assistant</h2>
README-LLM.md,574,
README-LLM.md,575,            <form onSubmit={handleSubmit}>
README-LLM.md,576,                <input
README-LLM.md,577,"                    type=""text"""
README-LLM.md,578,                    value={query}
README-LLM.md,579,                    onChange={(e) => setQuery(e.target.value)}
README-LLM.md,580,"                    placeholder=""Describe the heating system problem or ask a question..."""
README-LLM.md,581,"                    className=""query-input"""
README-LLM.md,582,                />
README-LLM.md,583,"                <button type=""submit"" disabled={loading || !query}>"
README-LLM.md,584,                    {loading ? 'Processing...' : 'Submit'}
README-LLM.md,585,                </button>
README-LLM.md,586,            </form>
README-LLM.md,587,
README-LLM.md,588,            {response && (
README-LLM.md,589,"                <div className=""response-container"">"
README-LLM.md,590,                    <h3>Diagnostic Response:</h3>
README-LLM.md,591,"                    <div className=""response-content"">"
README-LLM.md,592,"                        {response.split('\n').map((line, i) => ("
README-LLM.md,593,                            <p key={i}>{line}</p>
README-LLM.md,594,                        ))}
README-LLM.md,595,                    </div>
README-LLM.md,596,                </div>
README-LLM.md,597,            )}
README-LLM.md,598,        </div>
README-LLM.md,599,    );
README-LLM.md,600,}
README-LLM.md,601,
README-LLM.md,602,export default FaultFindingAssistant;
README-LLM.md,603,```
README-LLM.md,604,
README-LLM.md,605,### 2. Server-Side API
README-LLM.md,606,
README-LLM.md,607,Example Node.js API endpoint for handling queries:
README-LLM.md,608,
README-LLM.md,609,```javascript
README-LLM.md,610,// api/fault-finding/query.js
README-LLM.md,611,import { createClient } from '@supabase/supabase-js';
README-LLM.md,612,"import { Configuration, OpenAIApi } from 'openai';"
README-LLM.md,613,
README-LLM.md,614,// Initialize clients
README-LLM.md,615,const supabase = createClient(
README-LLM.md,616,"    process.env.SUPABASE_URL,"
README-LLM.md,617,    process.env.SUPABASE_SERVICE_KEY
README-LLM.md,618,);
README-LLM.md,619,
README-LLM.md,620,const configuration = new Configuration({
README-LLM.md,621,"    apiKey: process.env.OPENAI_API_KEY,"
README-LLM.md,622,});
README-LLM.md,623,const openai = new OpenAIApi(configuration);
README-LLM.md,624,
README-LLM.md,625,"export default async function handler(req, res) {"
README-LLM.md,626,    if (req.method !== 'POST') {
README-LLM.md,627,        return res.status(405).json({ error: 'Method not allowed' });
README-LLM.md,628,    }
README-LLM.md,629,
README-LLM.md,630,    try {
README-LLM.md,631,        const { query } = req.body;
README-LLM.md,632,
README-LLM.md,633,        // 1. Analyze query
README-LLM.md,634,        const queryAnalysis = await analyzeQuery(query);
README-LLM.md,635,
README-LLM.md,636,        // 2. Retrieve relevant information
README-LLM.md,637,"        const retrievedData = await retrieveInformation(queryAnalysis, query);"
README-LLM.md,638,
README-LLM.md,639,        // 3. Generate response
README-LLM.md,640,"        const response = await generateResponse(query, queryAnalysis, retrievedData);"
README-LLM.md,641,
README-LLM.md,642,        return res.status(200).json({ response });
README-LLM.md,643,    } catch (error) {
README-LLM.md,644,"        console.error('Error processing fault finding query:', error);"
README-LLM.md,645,        return res.status(500).json({ error: 'Error processing your request' });
README-LLM.md,646,    }
README-LLM.md,647,}
README-LLM.md,648,
README-LLM.md,649,async function analyzeQuery(query) {
README-LLM.md,650,    // Similar to the process_query function above
README-LLM.md,651,    // ...
README-LLM.md,652,}
README-LLM.md,653,
README-LLM.md,654,"async function retrieveInformation(queryAnalysis, originalQuery) {"
README-LLM.md,655,    // Implement retrieval based on query type
README-LLM.md,656,    // ...
README-LLM.md,657,}
README-LLM.md,658,
README-LLM.md,659,"async function generateResponse(query, queryAnalysis, retrievedData) {"
README-LLM.md,660,    // Generate the final response using LLM
README-LLM.md,661,    // ...
README-LLM.md,662,}
README-LLM.md,663,```
README-LLM.md,664,
README-LLM.md,665,## Performance Optimization
README-LLM.md,666,
README-LLM.md,667,### 1. Caching Strategy
README-LLM.md,668,
README-LLM.md,669,Implement caching for common queries:
README-LLM.md,670,
README-LLM.md,671,```javascript
README-LLM.md,672,// Simple in-memory cache
README-LLM.md,673,const queryCache = new Map();
README-LLM.md,674,const CACHE_TTL = 24 * 60 * 60 * 1000; // 24 hours
README-LLM.md,675,
README-LLM.md,676,function getCachedResponse(query) {
README-LLM.md,677,    const normalizedQuery = normalizeQuery(query);
README-LLM.md,678,    const cachedItem = queryCache.get(normalizedQuery);
README-LLM.md,679,
README-LLM.md,680,    if (cachedItem && (Date.now() - cachedItem.timestamp) < CACHE_TTL) {
README-LLM.md,681,        return cachedItem.response;
README-LLM.md,682,    }
README-LLM.md,683,
README-LLM.md,684,    return null;
README-LLM.md,685,}
README-LLM.md,686,
README-LLM.md,687,"function cacheResponse(query, response) {"
README-LLM.md,688,    const normalizedQuery = normalizeQuery(query);
README-LLM.md,689,"    queryCache.set(normalizedQuery, {"
README-LLM.md,690,"        response,"
README-LLM.md,691,        timestamp: Date.now()
README-LLM.md,692,    });
README-LLM.md,693,}
README-LLM.md,694,
README-LLM.md,695,function normalizeQuery(query) {
README-LLM.md,696,    // Normalize query to increase cache hits
README-LLM.md,697,"    return query.toLowerCase().trim().replace(/\s+/g, ' ');"
README-LLM.md,698,}
README-LLM.md,699,```
README-LLM.md,700,
README-LLM.md,701,### 2. Batch Processing
README-LLM.md,702,
README-LLM.md,703,For document ingestion:
README-LLM.md,704,
README-LLM.md,705,```javascript
README-LLM.md,706,"async function batchProcessDocuments(documents, batchSize = 5) {"
README-LLM.md,707,    for (let i = 0; i < documents.length; i += batchSize) {
README-LLM.md,708,"        const batch = documents.slice(i, i + batchSize);"
README-LLM.md,709,        await Promise.all(batch.map(doc => processDocument(doc)));
README-LLM.md,710,        console.log(`Processed batch ${Math.floor(i/batchSize) + 1}/${Math.ceil(documents.length/batchSize)}`);
README-LLM.md,711,    }
README-LLM.md,712,}
README-LLM.md,713,```
README-LLM.md,714,
README-LLM.md,715,## Maintenance and Updates
README-LLM.md,716,
README-LLM.md,717,### 1. Regular Reindexing
README-LLM.md,718,
README-LLM.md,719,Schedule periodic reindexing of documents:
README-LLM.md,720,
README-LLM.md,721,```javascript
README-LLM.md,722,// Update embeddings for all documents
README-LLM.md,723,async function updateAllEmbeddings() {
README-LLM.md,724,    const { data: chunks } = await supabase
README-LLM.md,725,        .from('document_chunks')
README-LLM.md,726,"        .select('id, content');"
README-LLM.md,727,
README-LLM.md,728,    console.log(`Updating embeddings for ${chunks.length} chunks...`);
README-LLM.md,729,
README-LLM.md,730,    const batchSize = 10;
README-LLM.md,731,    for (let i = 0; i < chunks.length; i += batchSize) {
README-LLM.md,732,"        const batch = chunks.slice(i, i + batchSize);"
README-LLM.md,733,        await Promise.all(batch.map(async (chunk) => {
README-LLM.md,734,            const embeddingResponse = await openai.createEmbedding({
README-LLM.md,735,"                model: ""text-embedding-3-small"","
README-LLM.md,736,"                input: chunk.content,"
README-LLM.md,737,            });
README-LLM.md,738,            const embedding = embeddingResponse.data.data[0].embedding;
README-LLM.md,739,
README-LLM.md,740,            await supabase
README-LLM.md,741,                .from('document_chunks')
README-LLM.md,742,                .update({ embedding })
README-LLM.md,743,"                .eq('id', chunk.id);"
README-LLM.md,744,        }));
README-LLM.md,745,
README-LLM.md,746,        console.log(`Updated batch ${Math.floor(i/batchSize) + 1}/${Math.ceil(chunks.length/batchSize)}`);
README-LLM.md,747,"        await new Promise(r => setTimeout(r, 500)); // Respect rate limits"
README-LLM.md,748,    }
README-LLM.md,749,}
README-LLM.md,750,```
README-LLM.md,751,
README-LLM.md,752,### 2. Content Refresh
README-LLM.md,753,
README-LLM.md,754,Process for adding new documents:
README-LLM.md,755,
README-LLM.md,756,```javascript
README-LLM.md,757,async function addNewDocument(filePath) {
README-LLM.md,758,    // Check if document already exists
README-LLM.md,759,"    const documentName = path.basename(filePath, '.md');"
README-LLM.md,760,    const { data: existingChunks } = await supabase
README-LLM.md,761,        .from('document_chunks')
README-LLM.md,762,        .select('id')
README-LLM.md,763,"        .eq('document_name', documentName);"
README-LLM.md,764,
README-LLM.md,765,    if (existingChunks && existingChunks.length > 0) {
README-LLM.md,766,        // Delete existing chunks
README-LLM.md,767,        await supabase
README-LLM.md,768,            .from('document_chunks')
README-LLM.md,769,            .delete()
README-LLM.md,770,"            .eq('document_name', documentName);"
README-LLM.md,771,
README-LLM.md,772,        console.log(`Deleted ${existingChunks.length} existing chunks for ${documentName}`);
README-LLM.md,773,    }
README-LLM.md,774,
README-LLM.md,775,    // Process the new document
README-LLM.md,776,    // (reuse document processing code from earlier)
README-LLM.md,777,}
README-LLM.md,778,```
README-LLM.md,779,
README-LLM.md,780,## Monitoring and Analytics
README-LLM.md,781,
README-LLM.md,782,Track query performance and user interactions:
README-LLM.md,783,
README-LLM.md,784,```javascript
README-LLM.md,785,"async function logQuery(query, retrievalStats, responseTime, userId = 'anonymous') {"
README-LLM.md,786,    await supabase.from('query_logs').insert({
README-LLM.md,787,"        query_text: query,"
README-LLM.md,788,"        user_id: userId,"
README-LLM.md,789,"        timestamp: new Date().toISOString(),"
README-LLM.md,790,"        retrieval_stats: retrievalStats,"
README-LLM.md,791,"        response_time_ms: responseTime,"
README-LLM.md,792,"        embedding_model: 'text-embedding-3-small',"
README-LLM.md,793,        llm_model: 'gpt-4'
README-LLM.md,794,    });
README-LLM.md,795,}
README-LLM.md,796,
README-LLM.md,797,// Use in API handler
README-LLM.md,798,const startTime = Date.now();
README-LLM.md,799,// ... process query ...
README-LLM.md,800,"const retrieval = await retrieveInformation(queryAnalysis, query);"
README-LLM.md,801,// ... generate response ...
README-LLM.md,802,const endTime = Date.now();
README-LLM.md,803,"await logQuery(query, {"
README-LLM.md,804,"    chunks_retrieved: retrieval.chunks.length,"
README-LLM.md,805,"    top_similarity: retrieval.chunks[0]?.similarity || 0,"
README-LLM.md,806,    query_type: queryAnalysis.query_type
README-LLM.md,807,"}, endTime - startTime, req.user?.id);"
README-LLM.md,808,```
README-LLM.md,809,
README-LLM.md,810,## Conclusion
README-LLM.md,811,
README-LLM.md,812,"This implementation provides a robust framework for integrating the heating system fault finding guides with Large Language Models using Supabase and vector embeddings. The system allows for both semantic searching and structured relationship queries, providing comprehensive answers to diagnostic and informational queries about heating systems."
README-LLM.md,813,
README-LLM.md,814,For optimal results:
README-LLM.md,815,
README-LLM.md,816,1. Regularly update the database as new documentation is created
README-LLM.md,817,2. Monitor query patterns to identify gaps in content
README-LLM.md,818,3. Refine embeddings and chunking strategies based on performance
README-LLM.md,819,4. Consider implementing user feedback mechanisms to improve accuracy
README-LLM.md,820,5. Periodically review and update relationship mappings between entities
README-LLM.md,821,
README-LLM.md,822,"By following this implementation guide, you can create a powerful LLM-based assistant for heating system diagnostics that leverages your comprehensive documentation to provide accurate, contextual troubleshooting assistance."
